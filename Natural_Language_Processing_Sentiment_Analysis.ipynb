{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SayiaFelix/natural-language-processing/blob/main/Natural_Language_Processing_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = ':https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F39657%2F61725%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240802%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240802T084738Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6db3ba8f7d3864ce839c6fd521ec1347388b098a44444982a93a1491bf8b8d2e452d206c3b5c811830dd092f4a04f45e824e6fa35233f69922fee9a8c3358db295deb38d2f43b1147fdc3f34faf40cc0ad11e7abbf82c377d19113a66eb938d7ccf4e18656e4a17459e8ff58c81baed98e6f0e977b3efac95bdd61cd486c768f152523212bd9b508623f160a0a0941f52dd29540d36a9d58c44fdf750496fae5ea5155b3131061990a83342641adb38826e34db2be8040e3e6d9084f69cf634890aed20076f68740df0c3150bfba2612d76d6d65abc1781af959f697b79df2e7cee384ceb5ef5879305d9db1d1af6d1917193cb93275e43c9236472a5a4be627'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "DCAARKuURgsX",
        "outputId": "1ff43695-5a0f-451b-ccde-c1003eb0061a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading , 167607 bytes compressed\n",
            "[==================================================] 167607 bytes downloaded\n",
            "Downloaded and uncompressed: \n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true,
        "id": "IuNHvG1URgsg",
        "outputId": "d127084b-7b82-43ff-eb89-764da6d90d9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import nltk\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"../input\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['amazon_alexa.tsv']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "53134ca38796b8b1ab1ca46b77051323f365350e",
        "id": "AnCKO-ZJRgsh"
      },
      "cell_type": "code",
      "source": [
        "#Using rstrip() plus a list comprehension to get a list of all the lines of text review"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true,
        "id": "YsbOe7S1Rgsi",
        "outputId": "1cda2748-72cb-4102-950a-0915b996afb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "df_review = [line.rstrip() for line in open('../input/amazon_alexa.tsv')]\n",
        "print (len(df_review))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3151\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e856c0880bb8207b027b9cffe8e27a353c761ed",
        "collapsed": true,
        "id": "2cmz-xO3Rgsj"
      },
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "df_review = pandas.read_csv('../input/amazon_alexa.tsv', sep='\\t')\n",
        "df_review.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "137240e8761c49c853752e10fbb16c930a641258",
        "id": "YRw5qlpURgsk"
      },
      "cell_type": "markdown",
      "source": [
        "**Basic Exploratory Data Analysis**\n",
        "Let's check out some of the stats with some plots and the built-in methods in pandas!"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "060c061ced7736842fc88e259c1c738082edad0c",
        "collapsed": true,
        "id": "J6KbhnNlRgsm"
      },
      "cell_type": "code",
      "source": [
        "df_review.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc744411e5c7aa65d241a98b1a5a977edead395c",
        "collapsed": true,
        "id": "DB1qItK_Rgsn"
      },
      "cell_type": "code",
      "source": [
        "df_review.groupby('rating').describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "19401d03ebbb50fb9cda62895954bbeaf740f141",
        "id": "QtsU8C9HRgso"
      },
      "cell_type": "markdown",
      "source": [
        "Making a new column to detect how long the text messages are:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f64036bdce4131525819905e4631d1740c83840",
        "collapsed": true,
        "id": "Gq-FSbKqRgsp"
      },
      "cell_type": "code",
      "source": [
        "df_review['length'] = df_review['verified_reviews'].apply(len)\n",
        "df_review.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d8bd656ede119fb4bcedcbda51d0958fab415494",
        "id": "mvkletFNRgsq"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06493d593a725f2cd6cfcb504f8abadaa7441f7b",
        "collapsed": true,
        "id": "ooLlJ1sgRgsr"
      },
      "cell_type": "code",
      "source": [
        "df_review['length'].plot(bins=50, kind='hist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f0961050034e598549c595e92e50d89a2274bc31",
        "id": "DM0b19RTRgsr"
      },
      "cell_type": "markdown",
      "source": [
        "Finding out the Maximum length review"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fd1c54b1006fd238fc4402c865574d330bbaaa92",
        "collapsed": true,
        "id": "EjYvjOsCRgss"
      },
      "cell_type": "code",
      "source": [
        "df_review.length.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "606517abd7e92d0570fbfd2312721f9d0a9f9c05",
        "id": "Lx0UVU_qRgss"
      },
      "cell_type": "markdown",
      "source": [
        "Wow! 2851 characters long review, let's use masking to find this message:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "26143937f362ae39317efadb15a8b5ff631614c2",
        "collapsed": true,
        "id": "WQ2CpMr-Rgss"
      },
      "cell_type": "code",
      "source": [
        "df_review[df_review['length'] == 2851]['verified_reviews'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1341e2fb3ca5b9ee02dc4eab9018594e8fbde8c0",
        "id": "PefHCs1MRgss"
      },
      "cell_type": "markdown",
      "source": [
        "**Looks like we have some sort of Essay writing review! Now let's focus back on the idea of trying to see if review length is a distinguishing feature between positive and negative review:**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f40788e9e0224efe2a2086d7e5314be793a9843c",
        "collapsed": true,
        "id": "GeeKNtAvRgst"
      },
      "cell_type": "code",
      "source": [
        "df_review.hist(column='length', by='feedback', bins=50,figsize=(10,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "40658a032c71f6c68f63337f6ecf84c62c2000b5",
        "id": "1-LpLcUrRgst"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98c0db0aa48a5f305c503c2bfc299bde3cc8f4f1",
        "collapsed": true,
        "id": "r_HV7aIdRgst"
      },
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "dataset = pd.read_csv('../input/amazon_alexa.tsv', delimiter = '\\t', quoting = 3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0f285f2fe29b757845bad85fa8086dbd82f6847",
        "collapsed": true,
        "id": "da8ZfzUoRgst"
      },
      "cell_type": "code",
      "source": [
        "# Cleaning the texts\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus=[]\n",
        "for i in range(0,3150):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['verified_reviews'][i] )\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    ps=PorterStemmer()\n",
        "    review=[ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review=' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "39013c4f1f5547a37ad85d236a9175fb5cf10cd8",
        "id": "CSJe7JCDRgsu"
      },
      "cell_type": "code",
      "source": [
        "# creating the Bag of words Model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer(max_features=1500)\n",
        "X=cv.fit_transform(corpus).toarray()\n",
        "y=dataset.iloc[:,4].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f24caab8b908667f9dd57f2dcd7bd46504763b5a",
        "collapsed": true,
        "id": "SXoIHv3fRgsu"
      },
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31300f44fc08ad137ebb3a82de5f2b519137dfa8",
        "collapsed": true,
        "id": "p__L57AsRgsu"
      },
      "cell_type": "code",
      "source": [
        "# Fitting Random Forest classifier with 100 trees to the Training set\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f4db7cf36132ba15cd745edd3acb2aff85a9c914",
        "id": "12_KtahSRgsu"
      },
      "cell_type": "code",
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "75c4c96817546c4143cba40fd8878dffed7d9761",
        "id": "dUv03HWVRgsv"
      },
      "cell_type": "code",
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19ef28dc22ab703b5e205aee6c05ebc0e9e89783",
        "id": "9jaT9E7_Rgsv"
      },
      "cell_type": "code",
      "source": [
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8773b5fbb83d83deb7714c38fbba8a78acea7b07",
        "id": "wEhIJ4KLRgsv"
      },
      "cell_type": "markdown",
      "source": [
        "Above shown matrix is known as confusion matrix and below is my result with accuracy of 94.28% and F1 Score of 0.9696.The Random Classifier Algo with 100 trees works efficiently to train the machine in predicting positive and negative reviews.\n",
        "<img src=\"https://image.ibb.co/hwRArK/claass.png\" alt=\"claass\" border=\"0\">"
      ]
    },
    {
      "metadata": {
        "_uuid": "e748a9037e40c4d716cc9fe8f8107cd80191a7ab",
        "id": "W5HLa1XhRgsw"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://image.ibb.co/jK2Byz/Precisionrecall.png\" alt=\"Precisionrecall\" border=\"0\">"
      ]
    },
    {
      "metadata": {
        "_uuid": "10e648f91e01a5c4616a2c2455c89b25cea6052b",
        "id": "TGkIx3cnRgsw"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://image.ibb.co/bU3ZJz/confusion_matrix_1.png\" alt=\"confusion_matrix_1\" border=\"0\">"
      ]
    },
    {
      "metadata": {
        "_uuid": "fe5895708381ffed22f4060f8fc065f57a36cf5c",
        "id": "SXkSmaKwRgsw"
      },
      "cell_type": "markdown",
      "source": [
        "**Can You more efficiently train your machine model and Beat my F1 score of 0.9696 using any other optimized classifier algo**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e6d5357b15c25068a8ed0021d777f64a4f3b006a",
        "id": "XRIJ5F-KRgsx"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Natural Language Processing : Sentiment Analysis",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}